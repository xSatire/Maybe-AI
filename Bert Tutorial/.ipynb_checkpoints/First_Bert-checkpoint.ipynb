{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6683eb80-f43d-49a0-8e1a-3a1b5bcc67fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"data.jsonl\"\n",
    "\n",
    "jsonObj = pd.read_json(path_or_buf=file_path, lines=True)\n",
    "jsonObj.set_index(\"key\",inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f237833-9d00-49c3-8acf-a2ff0ef0e633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>tool</th>\n",
       "      <th>heading</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Heading is one five zero, target is green comm...</td>\n",
       "      <td>electromagnetic pulse</td>\n",
       "      <td>150</td>\n",
       "      <td>green commercial aircraft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heading is two six zero, target is black, whit...</td>\n",
       "      <td>surface-to-air missiles</td>\n",
       "      <td>260</td>\n",
       "      <td>black, white, and yellow commercial aircraft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Heading is one zero five, target is silver, gr...</td>\n",
       "      <td>anti-air artillery</td>\n",
       "      <td>105</td>\n",
       "      <td>silver, green, and yellow light aircraft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heading is two niner zero, target is brown and...</td>\n",
       "      <td>electromagnetic pulse</td>\n",
       "      <td>290</td>\n",
       "      <td>brown and blue cargo aircraft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Heading is zero one five, target is yellow cam...</td>\n",
       "      <td>EMP</td>\n",
       "      <td>15</td>\n",
       "      <td>yellow camouflage drone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            transcript  \\\n",
       "key                                                      \n",
       "0    Heading is one five zero, target is green comm...   \n",
       "1    Heading is two six zero, target is black, whit...   \n",
       "2    Heading is one zero five, target is silver, gr...   \n",
       "3    Heading is two niner zero, target is brown and...   \n",
       "4    Heading is zero one five, target is yellow cam...   \n",
       "\n",
       "                        tool  heading  \\\n",
       "key                                     \n",
       "0      electromagnetic pulse      150   \n",
       "1    surface-to-air missiles      260   \n",
       "2         anti-air artillery      105   \n",
       "3      electromagnetic pulse      290   \n",
       "4                        EMP       15   \n",
       "\n",
       "                                           target  \n",
       "key                                                \n",
       "0                       green commercial aircraft  \n",
       "1    black, white, and yellow commercial aircraft  \n",
       "2        silver, green, and yellow light aircraft  \n",
       "3                   brown and blue cargo aircraft  \n",
       "4                         yellow camouflage drone  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31f8347f-c712-459c-b523-5c1032bfe32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonObj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4d35907-7c16-4221-b0cf-56e70d9fe022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 525kB/s]\n",
      "D:\\Programming\\envs\\ML\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\darry\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "tokenizer_config.json: 100%|██████████| 48.0/48.0 [00:00<00:00, 10.8kB/s]\n",
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 103kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "print(\"Loading BERT tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6d3489d-08d6-475a-b0a6-7e59130a3682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Heading is one five zero, target is green commercial aircraft, tool to deploy is electromagnetic pulse.\n",
      "Tokenized: ['heading', 'is', 'one', 'five', 'zero', ',', 'target', 'is', 'green', 'commercial', 'aircraft', ',', 'tool', 'to', 'deploy', 'is', 'electromagnetic', 'pulse', '.']\n",
      "Token IDs: [5825, 2003, 2028, 2274, 5717, 1010, 4539, 2003, 2665, 3293, 2948, 1010, 6994, 2000, 21296, 2003, 17225, 8187, 1012]\n"
     ]
    }
   ],
   "source": [
    "sentence = jsonObj.transcript[0]\n",
    "print(f'Original: {sentence}')\n",
    "\n",
    "#Print the tweet split into tokens\n",
    "print(f'Tokenized: {tokenizer.tokenize(sentence)}')\n",
    "\n",
    "#Print the tweet mapped to token ids\n",
    "print(f'Token IDs: {tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))}')\n",
    "\n",
    "#In actual situation, we use tokenize.encode to convert it into Token IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "412fe039-7936-41d3-87d8-18716abfb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = jsonObj.transcript"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca66cc0b-a0ff-47e4-a666-ce05981b6f9c",
   "metadata": {},
   "source": [
    "# Required Formatting\n",
    "\n",
    "* Add special tokens to the start and end of the sentence\n",
    "* Pad and Truncate all sentences to a single constant length\n",
    "* Explicitly differentiate real tokens from padding tokens with the \"attention mask\"\n",
    "\n",
    "## Special Tokens\n",
    "* At the end of every sentence, we need to append [SEP] token\n",
    "* This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine sometihing\n",
    "\n",
    "* At the start of each sentence, we must prepend the special [CLS] token, this token is used by the classfier\n",
    "\n",
    "## Sentence Length & Attention Mask\n",
    "* Bert has two constraints:\n",
    "  1. All sentences must be padded or truncated to a single, fixed length\n",
    "  2. The maximum sentence length is 512 token\n",
    "  3. [PAD] is used to fill in the blank spaces\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e0968ba-a5e7-4562-9271-768ff21512ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Lenghth: 30\n"
     ]
    }
   ],
   "source": [
    "#Testing for maximum length\n",
    "\n",
    "max_len = 0\n",
    "\n",
    "for length in question[:10]:\n",
    "\n",
    "    #Tokenize the text and add [CLS] and [SEP] tokens\n",
    "    input_ids = tokenizer.encode(length, add_special_tokens=True)\n",
    "\n",
    "    #update max length\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "\n",
    "    \n",
    "print(f'Max Lenghth: {max_len}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec24df-f8f7-41fa-8880-eda4d9a5aef9",
   "metadata": {},
   "source": [
    "## tokenizer.encode_plus\n",
    "\n",
    "1. Split the sentence into tokens\n",
    "2. Add the special [CLS] and [SEP] tokens\n",
    "3. Map the tokens to their IDs\n",
    "4. Pad or truncate all sentences to the same length\n",
    "5. Create the attention masks which explicitly differentiate tokens from [PAD] token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3242826-302e-47f2-a069-3174088d58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b7dcaf6-80c2-43ab-85e1-ece9d5422c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  5825,  2003,  2028,  2274,  5717,  1010,  4539,  2003,  2665,\n",
      "          3293,  2948,  1010,  6994,  2000, 21296,  2003, 17225,  8187,  1012,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(input_ids, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     24\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(attention_masks, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mlabels\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for quest in question:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        quest, #question to encode\n",
    "        add_special_tokens= True, #Add [CLS] and [SEP]\n",
    "        max_length=40,\n",
    "        truncation=True,\n",
    "        padding = \"max_length\",\n",
    "        return_attention_mask = True, #construct attention mask\n",
    "        return_tensors= 'pt' #Return pytorch Tensors\n",
    "    )\n",
    "\n",
    "    #Add encoded sentence to the list\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    #And its attention mask (simply differentiate padding from non-padding)\n",
    "    attention_masks.append(encoded_dict[\"attention_mask\"])\n",
    "\n",
    "print(input_ids[0])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "# labels = torch.tensor(labels) #labels are supposed to be target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6302f8d3-ec40-4825-858b-14bc6e545e97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
